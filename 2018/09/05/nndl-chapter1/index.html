<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2"/>
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.4.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="本文翻译自 Michael Nielson 的在线书籍 Neural Network and Deep Learning。感谢 Nielson 为我们带来如此全面高质量的内容。 1. 使用神经网络来识别手写数字人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：     大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主">
<meta name="keywords" content="translation,deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络和深度学习 第一章（翻译）">
<meta property="og:url" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/index.html">
<meta property="og:site_name" content="SuddenChive">
<meta property="og:description" content="本文翻译自 Michael Nielson 的在线书籍 Neural Network and Deep Learning。感谢 Nielson 为我们带来如此全面高质量的内容。 1. 使用神经网络来识别手写数字人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：     大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/digits.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/mnist_100_digits.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz0.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz1.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz2.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz3.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz4.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz5.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz6.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz7.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz8.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz9.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz9-1.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz9-2.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz10.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz11.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/digits.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/digits_separate.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/mnist_first_digit.png">
<meta property="og:image" content="https://suddenchive.github.io/2018/09/05/nndl-chapter1/tikz12.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/mnist_top_left_feature.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/mnist_other_features.png">
<meta property="og:image" content="https://suddenchive.github.io/images/nndl-chapter1/mnist_complete_zero.png">
<meta property="og:updated_time" content="2018-09-07T09:15:03.170Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络和深度学习 第一章（翻译）">
<meta name="twitter:description" content="本文翻译自 Michael Nielson 的在线书籍 Neural Network and Deep Learning。感谢 Nielson 为我们带来如此全面高质量的内容。 1. 使用神经网络来识别手写数字人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：     大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主">
<meta name="twitter:image" content="https://suddenchive.github.io/images/nndl-chapter1/digits.png">



  <link rel="alternate" href="/atom.xml" title="SuddenChive" type="application/atom+xml" />




  <link rel="canonical" href="https://suddenchive.github.io/2018/09/05/nndl-chapter1/"/>



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>神经网络和深度学习 第一章（翻译） | SuddenChive</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">SuddenChive</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br />首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br />关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br />分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://suddenchive.github.io/2018/09/05/nndl-chapter1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="SuddenChive">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SuddenChive">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络和深度学习 第一章（翻译）
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-09-05 10:00:00" itemprop="dateCreated datePublished" datetime="2018-09-05T10:00:00+08:00">2018-09-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-09-07 17:15:03" itemprop="dateModified" datetime="2018-09-07T17:15:03+08:00">2018-09-07</time>
              
            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>本文翻译自 <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielson</a> 的在线书籍 <a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="noopener">Neural Network and Deep Learning</a>。感谢 Nielson 为我们带来如此全面高质量的内容。</p>
<h2 id="1-使用神经网络来识别手写数字"><a href="#1-使用神经网络来识别手写数字" class="headerlink" title="1. 使用神经网络来识别手写数字"></a>1. 使用神经网络来识别手写数字</h2><p>人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：  </p>
<img src="/images/nndl-chapter1/digits.png" width="300" height="50" title="digits"> 
<p>大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主要的视觉皮层，常被称为 $V1$，其包含了 $1.4$ 亿个神经元以及这些神经元之间数百亿个连接。然而人类视觉不仅仅包含 $V1$，还有一整套的视觉皮层—— $V2, V3, V4$ 和 $V5$ ——  以完成逐渐复杂的图像处理任务。我们头上其实是顶了一台超级计算机的，这台超级计算机进过了 $1$ 亿年的进化演变，从而擅长理解视觉的世界。辨认手写的数字从不是一件容易的事情。<a id="more"></a>更准确地说，我们人类意外地擅长于使眼睛展现给我们的东西变得合理。但是这一切工作都是无意识就完成的，所以我们并不会感激我们的视觉系统解决了多么困难的问题。  </p>
<p>视觉模式识别这一问题的难度在当你尝试编写一个计算机程序来辨认上文中那样的数字时会变得格外明显。我们凭自己做起来很容易的事情突然之间变得很困难。一些简单的如何辨认形状的直观经验，比如 数字 $9$ 顶端是一个圈，而右下部分是一条竖直的线，似乎并不能很容易地在算法中表达。当你试图建立起像这样精确的规则时，你会很快地陷入大量异常、禁止和特殊情况的泥沼之中。看起来很绝望了。  </p>
<p>神经网络用了一种不同的方式尝试解决这个问题。其核心思想是，取出已有的大量的手写数字示例，也被称为训练示例，  </p>
<img src="/2018/09/05/nndl-chapter1/mnist_100_digits.png" title="mnist_100_digits">  
<p>然后开发一个能从这些示例中学习的系统。换句话说，神经网络利用这些示例来自动地推断出辨认手写数字的规则。更多地，通过增加训练示例的数量，神经网络能够学到更多关于手写的知识，从而提升自身的准确度。所以，就像你看到上面我列举了 $100$ 个训练示例的手写数字那样，或许我们能使用数千甚至上万、上亿级别的训练示例集来建立一个更好的手写数字辨认器。  </p>
<p>在本章中，我们将编写一个识别手写数字的神经网络的程序，其代码长度只有 $74$ 行，并且没有使用任何神经网络的代码库。这段简短的代码将能以 $96\%$ 的准确率在没有人工干涉的情况下辨认手写数字。更多地，我们在后续章节中将完善我们的思想以完成准确率超过 $99\%$ 的程序。事实上，在商业中神经网络的表现已经足够好了，以至被用于银行处理支票、邮局辨认邮编地址。  </p>
<p>我们将注重介绍手写数字的识别，因为这是我们在整体上学习神经网络的绝佳的原型问题。作为原型，它取得了一个很好地平衡点：它具有挑战性——即完成辨认手写数字的会是一项不小的成就——但它又没有困难到需要极端复杂的解决方式或者极为庞大的计算能力。更多地，它也是一种衍生出更多更为先进的技术的很好的方式，比如深度学习。所以，在阅读整本书的过程中，我们会不断回到辨认手写数字的问题上来。在之后的内容中，我们也会讨论这些思想将如何被运用到计算机视觉的其他问题中，以及类似的，自然语言处理和其他领域。  </p>
<p>当然了，如果本章的目的仅仅是编写一个辨认手写数字的计算机程序，那么本章的篇幅将会比现在短很多！所以在这个过程中，我们还会引出许多神经网络的关键思想，包括两种重要的人工神经元（感知器和 Sigmoid 神经元）和标准的神经网络的学习算法，即随机梯度下降算法。自始至终地，我都将关注于解释各个部分的内容为什么要被这样设计，以及建立你关于神经网络的直观感受。这些将需要比我仅仅介绍基本的技巧要多很多的讨论，但为了你能深刻地理解，这一切都是值得的。我们的收获会是，在本章结束之后，能理解深度学习是什么，并且为什么它很重要。   </p>
<h3 id="1-1-感知器"><a href="#1-1-感知器" class="headerlink" title="1.1. 感知器"></a>1.1. 感知器</h3><p>什么是神经网络？作为开始，我会描述一种人工神经元，它被称为感知器（<strong>Perceptrons</strong>）。感知器于 $1950s - 1960s$ 由 <strong>Frank Rosenblatt</strong> 研究并发展，而 <strong>Rosenlatt</strong> 受启发于先前 <strong>Warren McCulloch</strong> 和 <strong>Walter Pitts</strong> 的工作。现如今，更普遍的是使用其他人工神经元的模型，不管是在本书还是在现代的神经网络中。主要被使用的模型是  sigmoid神经元。我们将会很快介绍到 sigmoid。但为了理解为什么 sigmoid 神经元被设计成那样，我们需要先花时间理解感知器。  </p>
<p>那么，感知器是如何工作的呢？一个感知器，有若干个二进制输入 $x_1, x_2, …$， 产生一个二进制输出：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz0.png" title="tikz0">   
<p>在上图的例子中，感知器拥有三个输入 $x_1, x_2, x_3$ 。通常它可以有更多或者更少的输入。 <strong>Rossenblatt</strong> 提出了一种简单地计算输出的规则。他用到了权重 $w_1, w_2, … ,$ 这些实数来表示对应的输入对于输出的重要性。神经元的输出为 $0$ 或者 $1$，由加权和 $\sum_{j}w_jx_j$ 是小于还是大于某个阈值来决定。可以用以下代数用语来准确地表示：  </p>
<script type="math/tex; mode=display">
\tag{1} \label{1}
\begin{equation}
output = \left \{
\begin{array}{rcl}
  0 & & {if \ \  \sum_j w_j x_j \leq threshold}\\
  1 & & {if \ \  \sum_j w_j x_j > threshold}
  \end{array}
\right.
\end{equation}</script><p>这就是感知器工作原理的全部内容了！  </p>
<p>这是一个基本的数学模型。有一种思考感知器的方式是把它作为通过权衡已知证据来做出决策的装置。让我来举个例子。这不是一个很有现实意义的例子，但却很容易理解，并且我们也将很快会接触到更现实的例子。设想，周末即将来临，而你听说你的城市将举办一场芝士节。你非常喜欢芝士，于是得决定是否要去参加芝士节。你或许会权衡以下三个因素来做决策：</p>
<ol>
<li><p>周末的天气会不会很好？  </p>
</li>
<li><p>你的男朋友或者女朋友愿意陪你一起么？  </p>
</li>
<li><p>芝士节的举办地点离公共交通的站点距离近么？  </p>
</li>
</ol>
<p>我们能用对应的二进制变量 $x_1, x_2, x_3$ 来表示这三个因素。比如，如果周末天气不错，我们就让 $x_1 = 1$ ，如果天气很差，就让 $x_1 = 0$ 。类似的，当男朋友或女朋友也想去的时候，$x_2 = 1$， 反之，则 $x_2 = 0$。对 $x_3$ 和公共交通站点做类似的设置。  </p>
<p>现在，假设你超超超爱芝士的，爱到即使你的男朋友或女朋友不感兴趣，并且去芝士节的交通也很麻烦，你还是想要参加。但是或许你却无法忍受恶劣的天气，如果天气糟糕的话你就怎么都不愿意参加芝士节了。那么，你能使用感知器来为这类决策建立模型。一种方法是，令代表天气的权重 $w_1 = 6$， 其他因素的权重为 $w_2 = 2$ 和 $w_3 = 2$。$w_1$ 的数值越大，表示天气因素对你而言比男（或女）朋友是否愿意陪同或交通远近都要重要得多。最后，假设你将感知器的阈值设置为 $5$。有了以上的设置，感知器就实现了一个做决策的模型，只要天气好就会输出 $1$，天气差就会输出 $0$ 。男（或女）朋友是否愿意陪同，或是交通的远近，都无关紧要了。  </p>
<p>通过调整这些权重和阈值，我们能得到不同的决策模型。例如，假设我们将阈值改为 $3$。那么，感知器将权衡天气好坏、男（女）朋友是否愿意陪同和交通的远近来决定要不要参加芝士节。换句话说，它就不是最初那个决策模型了。降低阈值意味着你变得更愿意去参加芝士节。  </p>
<p>显然感知器才不会是人类决策的完整模型呢！但这个例子体现了一个感知器能如何权衡不同种类的已知因素来做决策。并且，让我们有理由相信，一个复杂的感知器网络能做出更加精确的决策：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz1.png" title="tikz1">   
<p>在这个网络中，第一列感知器——我们也称之为第一层感知器——通过权衡输入的因素来做出非常简单的决策。那第二层的感知器呢？这些感知器中的每一个都权衡第一层感知器决策的结果，从而做出决策。通过这样的方式，第二层的感知器能在与第一层感知器相比更复杂和抽象的层面上做出决策。同理，第三层的感知器又能做出更复杂的决策。这样，一个多层的感知器网络就能从事复杂又巧妙的决策制定。  </p>
<p>顺便提及，当我定义感知器时，我说过，一个感知器只有一个输出。而在上述感知器的网络中，看上去它们每个都有多个输出。事实上，它们仍然只有一个输出。多个输出箭头只是一种表明该感知器的输出被用来作为其它多个感知器的输入值的方式。这种方式比画一条输出箭头然后分裂出多条来要显得不那么奇怪吧。  </p>
<p>接下来，让我们简化一下对感知器的描述。条件 $\sum_j w_j x_j &gt; threshold$ 未免显得太过笨重，我们通过改变两处记号来简化它，首先，用点乘的形式 (dot product) 来简写 $\sum_j w_j x_j$，即令 $w \cdot x = \sum_j w_j x_j$，其中$w$ 和 $x$ 分别代表由权重和输入构成的向量；第二个改变是，将阈值移到不等式的另一边，并且用感知器的偏置来表示，即令 $b \equiv -threshold$。用偏置来代替阈值，感知器的规则就能被重写为：  </p>
<script type="math/tex; mode=display">
\tag{2} \label{2}
\begin{equation}
output = \left\{
    \begin{array}{rcl}
    0 & & {if \ \ w \cdot x + b \leq 0} \\
    1 & & {if \ \ w \cdot x + b > 0}
    \end{array}
\right.
\end{equation}</script><p>你可以认为偏置是让感知器输出 $1$ 的容易程度。或者用更生物的用语来表达，即偏置是让感知器激活的容易程度。对于一个拥有很大偏置的感知器，非常容易就让它输出 $1$ 了，但如果偏置是一个非常大的负数，那么这个感知器就很难输出 $1$。显然，引入偏置只是我们描述感知器时做出的小小的改变，但稍后我们会看到它能带来更多的记号上的简化。也因为如此，在本书剩下的部分，我们都不会使用阈值，而是改为使用偏置这个概念。  </p>
<p>我已经将感知器描述为一种权衡各种因素来做决策的方法。而感知器的另一个用途是计算基础的逻辑函数，即一些我们认为是底层的计算，像 AND， OR 和 NAND。例如，假设我们令 $ w_1 = w_2 = -2, b = 3 $ 。即下面的这个感知器：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz2.png" title="tikz2">  
<p>那么，我们发现，输入 $ 00 $ 将得到输出 $ 0 $ ，因为 $ (-2) \ast 0 + (-2) \ast 0 + 3 = 3 $ 大于 $ 0 $ 。请注意，这里我用了 $\ast$ 符号来表示普通的乘法运算。经过简单的计算，对于输入 $ 01 $ 和 $ 10 $ 也得到 $ 1 $ 。但对于输入 $ 11 $ ，却输出了 $ 0 $ ，因为 $ (-2) \ast 1 + (-2) \ast 1 + 3= -1 $ 小于 $ 0 $ 。所以，我们用这个感知器竟然实现了 NAND 门！  </p>
<p>NAND 门的例子表明我们能使用感知器来计算简单的逻辑函数。事实上，我们能使用感知器网络来计算任意逻辑函数。原因是，NAND 门对于逻辑运算来说是通用的，也就是说，我们能仅使用 NAND 门来构造所有的逻辑计算过程。举个例子，我们能使用 NAND门来实现将两个比特相加的运算电路。这要求计算位上的加和，即$ x_1 \oplus  x_2 $ ，也要计算进位，例如，当 $ x_1 $ 和 $ x_2 $ 都为 $ 1 $ 时，进位才为 $ 1 $ 。进位的值也就是位上相乘的值，即 $ x_1 x_2 $ ：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz3.png" title="tikz3">  
<p>为了得到等价的感知器网络，我们将所有的 NAND 门替换为有两个输入的感知器，并且其权重都为 $ -2 $ ，偏置为 $ 3 $ 。下图为替换后得到的网络。注意到，我稍微移动了一下对应于右下角 NAND 门的感知器，这样做仅仅是为了更容易地画图中的箭头符号。  </p>
<img src="/2018/09/05/nndl-chapter1/tikz4.png" title="tikz4">  
<p>这个感知器网络中一个吸引注意的地方是，最左端的感知器的输出作为最底端感知器的输入居然使用了两次。当我定义感知器模型的时候，我没说这种重复输出到同一个地方的情况是否被允许。但其实，这并不重要。如果我们不想允许这种情况，那么就简单地将两条代表输出的线——每条线上的权重都为 $ -2 $ ——合并成一条，然后把权重设置为 $ -4 $ 。（如果你没法显然得到这个等价的变换，你应该停下来先尝试证明一下。）经过这个修改，网络将会变成下面这个样子，所有没被标注出来的权重都是 $ -2 $ ，所有偏置都是 $ 3 $ ，其中有一个权重为 $ -4 $ ：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz5.png" title="tikz5">
<p>直到现在，我都把输入 $ x_1 $ 和 $ x_2 $ 当做变量悬放在感知器网络的左端。事实上，传统地做法是额外地画一层感知器——输入层——来对输入进行编码：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz6.png" title="tikz6">  
<p>这种只有一个输出而没有输入的输入感知器的记号，  </p>
<img src="/2018/09/05/nndl-chapter1/tikz7.png" title="tikz7">  
<p>是一种简写。它并不真的表示存在一个没有输入的感知器。为了解释这一点，假设我们确实有这样的感知器，它不接受任何输入。那么，加权和 $ \sum_j w_j x_j $ 会一直等于 $ 0 $ ，于是如果 $ b &gt; 0 $ 那么感知器就会输出 $ 1 $ ，$ b &lt; 0 $ 则输出 $ 0 $ 。也就是说，这个感知器仅仅会简单地输出一个修正值，而不是我们想要的值（上例中的 $ x_1 $ ）。最好认为输入感知器并不真的是一个感知器，而是一个简单的能输出我们期望的值 $x_1, x_2, …$ 的特殊元件。  </p>
<p>加法器的例子向我们展示了一个感知器网络能被用来模拟包含很多 NAND 门的电路。而因为 NAND 门具有逻辑计算的通用性，也就得出感知器也具有计算通用性的结论。  </p>
<p>了解到感知器的计算通用性是同时让人宽慰又失望的，宽慰是因为知道感知器网络能与其他计算设备一样强大；但失望是因为，看起来，感知器只不过是 NAND 门的另一种形式，这样的话感知器可难以称得上是一个大新闻！  </p>
<p>然而，情况比听起来的要好一些。事实表明，我们能够部署学习算法，来自动地调整人工神经元网络中的权重和偏置。作为对外部刺激的响应，这种调整无须编程者的干涉就能产生。这些学习算法使得我们能通过一种从根本上不同于传统逻辑门的方式来使用人工神经元。区别于单纯设计一个由 NAND 和其他逻辑门组成的电路，我们的神经元网络简单地通过学习来解决问题，有时这些问题如果直接用设计传统电路的方式会变得异常困难。  </p>
<h3 id="1-2-Sigmoid-神经元"><a href="#1-2-Sigmoid-神经元" class="headerlink" title="1.2. Sigmoid 神经元"></a>1.2. Sigmoid 神经元</h3><p>能学习的算法，听起来很吸引人不是嘛。但是会我们如何将这样的算法应用到一个神经网络中呢？假设我们有一个感知器网络，并且我们希望让它能学习解决某些问题。例如，这个网络的输入或许是来自一个手写数字的影印图片的行像素数据。我们希望网络学习权重和偏置以使得网络的输出能够正确地分类手写数字。为了观察学习的过程，假设我们对网络中的权重或偏置进行很小的修改。我们想要的结果是，对于这个很小的修改，仅会相应地引起网络的输出发生很小的变化。接下来我们会看到，这一性质将使得学习变为可能。大致上，我们想要的是下面这个样子（显然这对于完成手写数字的识别有点过于简单了！）：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz8.png" title="tikz8">
<p>如果对权重（或偏置）做出很小的修改时仅仅会引起输出的很小的改变，那么我们能利用这一点来修改权重和偏置来让网络的行为变得更像我们希望的那样。例如，假设网络错误地将数字 “$9$” 的图片识别为数字 “$8$” 。我们能计算出对权重和偏置做出多小的修改时，会让网络离将这张图片识别为正确地数字 “$9$”更近一步 。然后我们重复这个过程，不断地修改权重和偏置从而产生越来越好的输出。这就是网络的学习过程。  </p>
<p>问题是，如果我们的网络中包含感知器的话，以上的情况将不会发生。事实上，网络中的任意单个感知器，对于权重或偏置的很小的变化，有时会引起该感知器输出值的跳跃，比如，从 $0$ 变为 $1$ 。这个跳跃也许会造成网络的其他部分以一种复杂的方式发生完全的改变。所以，尽管现在 “$9$” 能被正确地分类，网络对于所有其他图片的识别结果也会以很难控制地方式完全地改变。这会让我们通过逐渐修改权重和偏置使得网络更靠近期望的行为变得困难。或许存在一些聪明的方法来避开这一问题。但如何让一个感知器网络达到学习的效果似乎还是毫无头绪。  </p>
<p>我们可以引入一种新的被称为 sigmoid 神经元的人工神经元来解决这一问题。Sigmoid 神经元类似于感知器，但也做了一些修改，从而使得对权重和偏置的细小修改只会对输出造成很小的改变。这是保证Sigmoid 神经元网络能够学习的关键因素。  </p>
<p>好的，那么让我来描述一下 sigmoid 神经元。我们将用与描述感知器一样的方式来表示 Sigmoid 神经元：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz9.png" title="tikz9">  
<p>就像感知器，sigmoid 神经元也有输入 $x_1, x_2, … $。但与输出 $ 0 $ 或者 $ 1 $ 不同，这些输入在 sigmoid 神经元上产生的可以是介于 $ 0 $ 和 $ 1 $ 之间的任意值。例如，$ 0.638 … $ 就是一个 sigmoid 神经元有效的输出。与感知器还类似的一点是， sigmoid 神经元对于每个输入，也有相应的权重 $ w_1, w_2, … $ ，和一个总体的偏置 $ b $ 。Sigmoid 的输出不是 $ 0 $ 或者 $ 1 $ ，而是 $ \sigma (w \cdot x + b) $ ，其中 $ \sigma $ 被称为 sigmoid 函数* <span class="marginnote">* 顺便提下，$ \sigma $ 有时也被称为逻辑函数，这种新的神经元的类型是逻辑神经元。记住这个术语很有用，因为这些术语被很多与神经网络打交道的人使用。然而，我们还是先专注于 sigmoid 。</span>，其定义如下：</p>
<script type="math/tex; mode=display">
\tag{3} \label{3}
\sigma(z) \equiv \frac{1}{1 + e^{-z}}.</script><p>更准确地，一个输入是 $x_1, x_2, …$，权重为 $w_1, w_2, …$ ，偏置为 $ b $ 的 sigmoid 神经元的输出为  </p>
<script type="math/tex; mode=display">
\tag{4} \label{4}
\frac{1}{1 + {\rm exp}(-\sum_{j}{w_j x_j} - b)}.</script><p>一眼看上去，sigmoid 神经元的长相跟感知器很不一样。Sigmoid 函数的代数形式看起来很不清晰明了，并且如果你对它不熟悉的话也会望而生畏。事实上，sigmoid 神经元与感知器之间有很多相似之处，另外，sigmoid 函数的代数形式更多地是技术细节而不是你理解上的障碍。  </p>
<p>为了理解其与感知器之间的相似处，假设 $ z \equiv w \cdot x + b $ 是一个很大的正数。那么 $ e ^ {-z} \approx 0 $ ，从而 $ \sigma (z) \approx 1 $ 。换句话说，当 $ z = w \cdot x + b $ 数值很大且符号为正，那么 sigmoid 神经元的输出近似为 $ 1 $ ，就跟感知器上发生的一样。相反地，假设 $ z = w \cdot x + b $ 是非常小的负数。那么 $ e ^ {-z} \rightarrow \infty $ ，从而 $ \sigma (z) \approx 0 $ 。所以，当 $ z = w \cdot  x + b $ 数值很大且符号为负，那么 sigmoid 神经元的行为也与感知器很相近。只有当 $ w \cdot x + b $ 的数值不太大的时候，才与感知器模型有很大的不同。  </p>
<p>那么关于 $ \sigma $ 的代数形式呢？我们又应该怎么理解它？事实上， $ \sigma $ 的具体形式不那么重要 —— 重要的是画出来的这个函数的图形。如下：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz9-1.png" title="tikz9-1">  
<p>这个图形是如下阶跃函数的平滑版：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz9-2.png" title="tikz9-2">  
<p>如果 $ \sigma $ 被定义为阶跃函数，那么 sigmoid 神经元就会成为一个感知器，因为其会根据 $ w \cdot x + b $ 的正负号来输出 $ 1 $ 或 $ 0 $<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> 。  </p>
<p>通过使用真正的 $ \sigma $ 函数，我们就能得到上述的一个平滑过的感知器。确实，$ \sigma $ 函数的平滑性才是关键所在，而不是它具体的形式。$ \sigma $ 的平滑性意味着，权重的细小改变 $ \triangle  w_j $ 与偏置的细小改变 $ \triangle b $ 将使得网络的输出产生一个细小改变 $ \triangle \rm output $ 。事实上，由微积分学理论可以知道，$ \triangle \rm output $ 能被很好地近似求得：  </p>
<script type="math/tex; mode=display">
\tag{5} \label{5}
\triangle {\rm output} \approx \sum_{j}{\frac{\partial \ {\rm output}}{\partial w_j} \triangle w_j} + \frac {\partial \ {\rm output}}{\partial b} \triangle b,</script><p>其中，求和是针对所有的权重 $ w_j $，而 $ \partial  {\rm output} / \partial w_j $ 和 $ \partial  {\rm output} / \partial b $ 表示 $ \rm output $ 分别关于 $ w_j $ 和 $ b $ 的偏微分。如果你不对付偏微分的话也不要感到惊慌！虽然上面这个表达式看起来很复杂，有一堆偏微分，它其实只是表达了很简单的一件事儿（而且是件很棒的事儿）：$ \triangle \rm output $ 是关于权重和偏置的变化，即 $ \triangle w_j $ 和 $ \triangle b $ 的线性函数。这种线性关系使得选择对权重和偏置进行细小修改从而让输出达到任意想要的细小改变这件事变得相当容易。所以即使 sigmoid 神经元与感知器在性质和行为上有很多相似之处，但它让计算权重和偏置的改变会如何影响其输出变得简单了许多。  </p>
<p>如果真的如之前所说， $ \sigma $ 的图像才是关键，而不是它的具体形式，那么我们为什么将 $ \sigma $ 定义为 $ \eqref{3} $ 这样呢？事实上，在本书后面的内容，我们会偶尔考虑其他类型的神经元，这些神经元的输出同样为 $ f( w \cdot x + b) $，只不过激活函数 $ f $ 的形式不同。当我们使用不同的激活函数，唯一有区别的一点是， $ \eqref{5} $ 中偏微分的具体值发生了改变。而经验表明，若使用现在的 $ \sigma $ 的定义，能简化计算这些偏微分的过程，因为指数函数求导时有一些迷人的性质。总之， $ \sigma $ 在神经网络中被普遍地使用，本书也用的最多。  </p>
<p>我们如何解释一个 sigmoid 神经元的输出呢？显然，感知器和 sigmoid 神经元的一个很大的区别在于，sigmoid 神经元不只输出数值 $ 0 $ 和 $ 1 $ ，类似 $ 0.173… $ 或 $ 0.689… $ 等等都是合法的输出。这一点很有用处，比如说，当我们想让输出值表示为输入到神经网络的一张图片上所有像素点的平均亮度。但有时这一特性又是件麻烦事。假设我们想让网络的输出指示“输入图片是张 $ 9 $” 或者“输入图片不是张 $ 9 $”。显然，如果输出只能是 $ 0 $ 或 $ 1 $ ，就像感知器，这件事会简单得多。  </p>
<p>但实践中，我们能人工设置一个约定来解决，例如，对于大于 $ 0.5 $ 的任意输出，都指示“输入图片是张 9”，而小于 $ 0.5 $的任意输出，都指示“不是 9“。每当我们使用类似约定时，我都会明确指出，所以不会造成你对此的任何疑惑。  </p>
<h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><ul>
<li><p><strong>用 sigmoid 神经元模拟感知器（第一部分）</strong><br>假设我们将感知器网络中所有的权重和偏置都乘上一个正的常数 $ c &gt; 0$。证明网络的行为不会发生变化。  </p>
</li>
<li><p><strong>用 sigmoid 神经元模拟感知器（第二部分）</strong>  </p>
<p>假设起始我们与上题一样，有一个感知器网络。同时假设网络的输入是给定的，这里不需要给定具体的输入值，只要求输入是我们想要的样子。这个网络中的任何感知器，对此输入 $ x $，其权重和偏置都满足 $ w \cdot x + b \neq 0 $ 。现在，将网络中的所有感知器都替换为 sigmoid 神经元，然后将所有权重和偏置都乘上一个正的常数 $ c &gt; 0 $ 。证明当极限 $ c \rightarrow \infty $ 成立时，这个 sigmoid 网络的行为与之前的感知器网络完全相同。而如果有任一感知器满足 $ w \cdot x + b = 0 $ ，这一结论就不成立了，为什么呢？  </p>
</li>
</ul>
<h3 id="1-3-神经网络的架构"><a href="#1-3-神经网络的架构" class="headerlink" title="1.3. 神经网络的架构"></a>1.3. 神经网络的架构</h3><p>在下一节中我将介绍一个能很好地分类手写数字的神经网络。作为准备工作，有必要解释一下我们用来给一个网络的不同部分命名的术语。假设我们有这样的网络：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz10.png" title="tikz10">  
<p>正如之前所说，网络中最左端的一层叫做输入层，并且这层的神经元叫做输入神经元。最右端或者说输出层包含了输出神经元，在这个例子中，只有一个输出神经元。中间那一层被称为隐藏层，因为这层里的神经元既不属于输入也不属于输出。“隐藏”这个词或许听起来有点儿神秘——我第一次听到这个词的时候还想着它肯定有什么深层次的哲学或数学上的重要性——但它真的就是“既不属于输入也不属于输出”的意思。上面这个网络只有一个隐藏层，但有些网络能有多个隐藏层。例如，下面拥有两个隐藏层的四层网络：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz11.png" title="tikz11">  
<p>然而颇具迷惑性的是，由于一些历史原因，像这样的多层网络有时候会被称为多层感知器或 MLPS ( Multilayer Perceptrons ) ，尽管组成它的都是 sigmoid 神经元而不是感知器。我不会在本书中使用 MLP 这个术语，因为这太有迷惑性了，但我想提示你这个术语的存在。  </p>
<p>网络的输入层和输出层的设计经常都很直接。例如，假设我们想要判断一张图片里的手写数字是否为”$ 9 $“。一种自然的设计方式是，将图片像素的亮度编码到输入神经元中。如果这张图片是一张 $ 64 \times 64 $ 的灰度图，那么我们就有 $ 4096 = 64 \times 64 $ 个输入神经元，并且它们的亮度被准确地缩放到 $ 0 $ 与 $ 1 $ 之间。输出层将只包含单个神经元，其输出值若小于 $ 0.5 $ 则表示“输入图片不是一张 $ 9 $”，若大于 $ 0.5 $，则表示“输入图片是一张 $ 9 $”。  </p>
<p>尽管网络的输入层和输出层的设计经常都很直接，但隐藏层的设计有时可以显得很有艺术性。尤其是，根本不可能用几条简单的经验法则来总结所有的隐藏层的设计流程。所以，神经网络的研究者们提出了许多设计隐藏层的启发式方法，从而让神经网络表现出人们想要的行为。例如，一些启发式方法能用来帮助决定如何在隐藏层神经元的数量和训练网络所需要的时间中折中。在后面的内容中，我们会遇到几种这样的启发式方法。  </p>
<p>直到现在，我们已经讨论了前一层神经元的输出会作为后一层的输入这种神经网络。这类网络我们称之为前馈神经网络。这也意味着，网络中不存在环形——信息都是向前传递的，而不会向后。如果有环形的话，会导致 $ \sigma ​$ 函数的输入取决于其输出。对这种情况很难赋予其实际意义，所以我们不允许这样的环形。  </p>
<p>然而，也有其他的神经网络模型，允许存在向后反馈的环。这些模型被称为循环神经网络。这种模型的基本思想是，让神经元的输出只保持有限的一段时间，之后神经元就陷入静止。其输出也能刺激其他的神经元，或许稍等片刻也能使其激活，同样产生只持续有限时间的输出。这样，刺激更多的神经元输出，经过一段时间，我们就能得到瀑布式的输出传递。环形并不会在这样的模型中带来问题，因为一个神经元的输出只会在过了一段时间之后才影响自身的输入，而不是立即生效。  </p>
<p>循环神经网络没有前馈网络那样有影响力，部分是因为循环神经网络的学习算法（至少到目前为止）不是那么强大。但循环网络还是极其有吸引力的。它们比前馈网络更接近我们大脑的工作方式。并且有可能，循环神经网络能解决一些前馈网络得用很高的复杂度才能完成的重要问题。但是，量力考虑的话，这本书中我们将只关注更被广泛使用的前馈网络。  </p>
<h3 id="1-4-一个分类手写数字的简单网络"><a href="#1-4-一个分类手写数字的简单网络" class="headerlink" title="1.4. 一个分类手写数字的简单网络"></a>1.4. 一个分类手写数字的简单网络</h3><p>在定义了神经网络之后，让我们回到手写识别这个问题上来。我们能将识别手写数字的问题分为两个子问题。首先，我们希望有一种方式能将一张包含许多数字的图片分割成一系列只包含一个数字的图片，例如，我们要将这张图片  </p>
<img src="/images/nndl-chapter1/digits.png" width="300" height="50" title="digits">   
<p>分成六张独立的图片  </p>
<img src="/images/nndl-chapter1/digits_separate.png" width="400" height="50" title="digits_separate">   
<p>我们人类很轻松就能解决这个图片的分割问题，但对于一个计算机程序来说，正确地将图片分割开会是一个挑战。一旦这张图片被分割开，之后程序就需要对每一个数字进行分类。举例来说，我们希望我们的程序识别上面的第一个数字，  </p>
<img src="/images/nndl-chapter1/mnist_first_digit.png" width="50" title="50mnist_first_digit">  
<p>为一个 $5$ 。  </p>
<p>我们将关注如何写一个程序来解决第二个问题，也就是，识别单独的数字。我们这样做因为，事实上分割问题不是那么困难，如果你能够很好地解决分类单独数字的话。有很多种解决分割问题的方法。一种方法是，尝试多种不同的分割图片的方式，对其结果，使用单独数字的分类器来评分。一个图片分割的结果，当单独数字的分类器能对其每个分割段都有确定的分类输出，会得到更高的评分；而当单独数字的分类器在对其中一个或者几个分割段遇到分类的困难时，这个分割方式就只能得到一个低分。其思想就是，如果分类器在某个地方遇到困难，可能是因为分割方式选择得不正确。这个思想和它的一些变种在用来解决分割问题上的效果还不错。所以与其担心分割问题，我们将集中注意力于开发一个神经网络来解决更有趣和困难的问题，亦即，识别单独的手写数字。  </p>
<p>为了识别单独的数字，我们将会使用三层结构的神经网络：  </p>
<img src="/2018/09/05/nndl-chapter1/tikz12.png" title="tikz12">  
<p>这个网络的输入层包含了编码输入像素点值的神经元。正如下一节将讨论的一样，对于这个网络，我们的训练数据将由许多 $ 28  \ast 28 $ 分辨率的手写数字的扫描图片，所以输入层包含  784 = 28 \ast 28  个神经元。为了简便，我在上图中省略了大部分的输入神经元。输入的像素值是灰度缩放值，即值为 $ 0.0 $ 时，表示白色，值为 $ 1.0 $ 时，表示黑色，中间的值表示灰度逐渐变黑。  </p>
<p>网络的第二层是隐藏层。我们用 $ n $ 表示隐藏层的神经元个数，并且将对 $ n $ 取不同的值来试验。这个示例中展示的是一个规模很小的隐藏层，只包含了 $ n = 15 $ 个神经元。  </p>
<p>网络的输出层包含 $ 10 ​$ 个神经元。如果第一个神经元被激活，比如 输出 $ output \approx 1​$ ，那么表明网络认为这个数字是一个 $ 0 $ 。如果第二个神经元激活那么表明这个网络认为这个数字是一个 $ 1 $ 。以此类推。更精确地说，我们用 $ 0 $ 到 $ 9 $ 来依次标记这些输出神经元，然后找出哪一个有最高的输出值。假如说，是标记为 $ 6 $ 的神经元，那么我们的网络将猜这个数字是 $ 6 $。对其他输出神经元也一样。  </p>
<p>或许你会想，为什么我们要用 $ 10 $ 个输出神经元。毕竟，网络的目标是告诉我们哪个数字 $(0, 1, 2, … , 9)$ 对应着输入图像。很自然地，我们能使用仅 $4$ 个输出神经元就能做到，只要将这四个神经元的每一个都当做二进制值，其数值取决于该神经元的输出是更接近 $0$ 还是 $1$ 。四个神经元来编码网络对我们的回答足够了，因为 $2^4 = 16$ ，比输入可能的 $10$ 个数值要多很多。那为什么我们反而要用 $10$ 个神经元呢。这不是很浪费么？根本原因来自于经验主义：我们可以都尝试这两种网络的设计，然后会看到，对于这个特定的问题，拥有 $10$ 个输出神经元的网络识别数字的学习效果比拥有 $4$ 个输出神经元的网络要好。但这又会让我们产生疑惑，为什么使用 $10$ 个输出神经元就会更好呢。有没有什么启发式的方法能提前告诉我们应该用 $10$ 个输出神经元来编码而不是 $4$ 个呢？  </p>
<p>为了理解为什么这样做，可以遵循第一条原则，即弄清楚神经网络到底在做什么。考虑第一种情况，我们使用 $10$ 个输出神经元。我们来关注第一个输出神经元，即试图确定输入数字是否为 $0$ 的神经元。它通过权衡来自隐藏层神经元的输出来做决策。那么那些隐藏层神经元在做什么呢？方便讨论起见，假设隐藏层的第一个神经元是来检测如下这张图片是否存在：  </p>
<img src="/images/nndl-chapter1/mnist_top_left_feature.png" width="100" height="100" title="mnist_top_left_feature">  
<p>提高输入图片与上图重叠部分的像素点的权重，而对于其他像素点，取较小的权重，这个神经元就能做到这一点。同样地，方便起见，我们假设隐藏层的第二、三和四个神经元分别检测下面这些图片是否存在：  </p>
<img src="/images/nndl-chapter1/mnist_other_features.png" width="300" height="100" title="mnist_other_features">  
<p>正如你所猜测的，这四张图片组合成之前一行数字中数字 $0$ 的图像：  </p>
<img src="/images/nndl-chapter1/mnist_complete_zero.png" width="100" height="100" title="mnist_complete_zero">  
<p>所以如果这四个隐藏层神经元全都被激活，那么我们就能得出结论，这个数字就是 $0$ 。当然，我们得出这种结论的方法肯定不止这一种——还有许多其他正当地的方法（就比如，通过将上述图像进行平移或者轻微地变换）。但看起来认定现在这种情况下我们得出输入是 $0$ 的结论还是不会有错的。  </p>
<p>假设神经网络是这样工作的，那么我们就能对为什么要有 $10$ 个输出神经元，而不是 $4$ 个，给出一个合理的解释了。如果我们只有 $4$ 个输出神经元，那么第一个输出神经元将要试图确定输入数字的最高位的数值是多少。而将最高位的数值和上面所展示的这种简单图形对应起来可不是一件容易的事儿。很难想象有什么好的历史原因让我们将数字的部分形状与它的最高位数值联系起来。  </p>
<p>现在，说的这些，也只是一个启发式的假想。没有证据表明，三层结构的神经网络就像我所描述的那样工作，即隐藏层神经元来检测简单的部分形状。或许存在一个聪明的学习算法能找出权重的数值从而让我们能只使用 $4$ 个输出神经元。但作为一个启发式的想法，思考我所描述的内容能起到很好的效果，也能让你在设计好的神经网络架构上省去很多的时间。  </p>
<h4 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h4><ul>
<li>有一种方法，通过在上述三层结构的网络中增加一层额外的神经元从而能够确定输入数字的二进制表达。这个额外的神经层将前一层的输出转换成一个二进制表达，如下图所示。找到这样的输出层的一组权重和偏置。假设前三层神经元能使得第三层（也就是之前的输出层）的正确输出的激活值至少大于 $0.99$，而不正确的输出，其激活值低于 $0.01$ 。  </li>
</ul>
<p>（未完待续）</p>
<p>其它章节如下：  </p>
<p><a href="https://suddenchive.github.io/2018/09/01/nndl-chapter0/">神经网络与深度学习 前言（翻译）</a>  </p>
<p><a href="https://suddenchive.github.io/2018/09/07/nndl-chapter2/">神经网络与深度学习 第二章（翻译）</a> </p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/translation/" rel="tag"># translation</a>
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/09/01/nndl-chapter0/" rel="next" title="神经网络和深度学习 前言（翻译）">
                <i class="fa fa-chevron-left"></i> 神经网络和深度学习 前言（翻译）
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/09/07/nndl-chapter2/" rel="prev" title="神经网络和深度学习 第二章（翻译）">
                神经网络和深度学习 第二章（翻译） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">SuddenChive</span>

  

  
</div>




  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.7.1</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.4.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.4.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.4.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.0"></script>



  



  










  





  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  
  

  

  

  

  

  


</body>
</html>
