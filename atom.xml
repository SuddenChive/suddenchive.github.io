<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SuddenChive</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://suddenchive.github.io/"/>
  <updated>2018-09-13T07:52:04.067Z</updated>
  <id>https://suddenchive.github.io/</id>
  
  <author>
    <name>SuddenChive</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>神经网络和深度学习 第二章（翻译）</title>
    <link href="https://suddenchive.github.io/2018/09/07/nndl-chapter2/"/>
    <id>https://suddenchive.github.io/2018/09/07/nndl-chapter2/</id>
    <published>2018-09-07T04:00:00.000Z</published>
    <updated>2018-09-13T07:52:04.067Z</updated>
    
    <content type="html"><![CDATA[<p>本文翻译自 <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> 的在线书籍 <a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="noopener">Neural Network and Deep Learning</a>。感谢 Nielson 为我们带来如此全面高质量的内容。  </p><h2 id="反向传播算法是如何实现的"><a href="#反向传播算法是如何实现的" class="headerlink" title="反向传播算法是如何实现的"></a>反向传播算法是如何实现的</h2><p>在上一章中，我们看到神经网络是如何使用梯度下降算法来学习权重和偏置的。但在讲述中遗留了一个问题：我们并没有讨论如何计算代价函数的梯度。这还真是个大问题呢！在这一章中，我会介绍一个快速计算这些梯度的算法，这个算法也被称为反向传播算法。  </p><p>反向传播算法于20世纪70年代被首次提出，但其重要性直到由 <a href="http://en.wikipedia.org/wiki/David_Rumelhart" target="_blank" rel="noopener">David Rumelhart</a>， <a href="http://www.cs.toronto.edu/~hinton/" target="_blank" rel="noopener">Geoffrey Hinton</a> 和 <a href="http://en.wikipedia.org/wiki/Ronald_J._Williams" target="_blank" rel="noopener">Ronald Williams</a> 撰写的一篇著名论文的发表才被世人发觉。这篇论文描述了一些神经网络，在这些网络中，反向传播算法会比之前的学习算法取得更高的效率，从而使得神经网络能解决一些先前无法解决的问题。如今，反向传播算法已经成为神经网络学习的中流砥柱。  </p><a id="more"></a><p>本章的数学内容会比书中其他章节更多。如果你并不痴迷于数学，你可以尝试跳过这一章节，然后把反向传播算法当作你忽略其具体内容的黑盒子。对啊，何苦要花费时间学习这些数学细节呢 o(╥﹏╥)o？  </p><p>其实，关键在于理解。反向传播的核心是一个有关代价函数 $C$ 的偏微分 $\partial C / \partial w$ 的表达式， 对于神经网络中所有的权重 $w$ （或偏置 $b$ ）而言。这个表达式能告诉我们当我们改变权重和偏置时代价函数变化的快慢。虽然这个表达式在一定程度上是很复杂的，但也体现了一种形式上的美感，即对于每个元素，都有一个自然的、直觉上的解释。所以，反向传播并不仅仅是一种学习的快速算法。它实际上能帮我们详细地洞察到改变权重和偏置会如何改变神经网络总体表现出的行为。这一点很值得让我们学习其具体内容了。  </p><p>我都这么说了，如果你仅想随便读读本章，或者直接跳到下一章内容，行吧 ┓( ´∀`)┏。我已经将本书剩余内容写得足够易懂，即使你不了解反向传播算法的具体细节。当然，之后会有几处内容引用到本章的结果。但在这些地方，即使你没弄懂所有的推理过程，你也能够理解其主要的结论。  </p><h3 id="热身：一种基于矩阵的计算神经网络输出的方法"><a href="#热身：一种基于矩阵的计算神经网络输出的方法" class="headerlink" title="热身：一种基于矩阵的计算神经网络输出的方法"></a>热身：一种基于矩阵的计算神经网络输出的方法</h3><p>在讨论反向传播之前，让我们学习一种基于矩阵的计算神经网络输出的方法来热热身。实际上，我们在上一章后面的部分已经简要地看过这个算法了，但我讲的很快，所以需要再仔细地回顾一下细节。尤其因为，这能让我们在已经了解过的情境下来熟悉反向传播算法中各种记号，无疑是一种绝佳的方式。  </p><p>让我们从一个很清晰明了的表示网络中权重的记号开始。我们使用 $w_{jk}^{l}$ 表示在第 $l-1$ 层中的第 $k$ 个神经元与第 $l$ 层中的第 $j$ 个神经元之间的连接上的权重。举个例子，下图描绘了网络中从第二层的第四个神经元到第三层的第二个神经元的连接上的权重：  </p><img src="/2018/09/07/nndl-chapter2/tikz16.png" title="tikz16">  <p>这种记号初看起来很笨拙，且需要花费一些功夫来掌握。但努力一番后你也会发现，它其实是简单又自然的。这种记号的一个奇怪的地方是，索引 $j$ 和 $k$ 的顺序。你可能会认为如果用 $j$ 来代表输入神经元、$k$ 来代表输出神经元会更加符合逻辑，而不是相反的情况，就像已经被定义的这样。我会在下面解释这个奇怪设置的缘由。  </p><p>我们使用一种类似的记号来表示网络的偏置和激活值。即显然地，我们使用 $b_{j}^{l}$ 表示 $l^{th}$ 层的 $j^{th}$ 神经元的偏置；使用 $a_{j}^{l}$ 表示 $l^{th}$ 层的 $j^{th}$ 神经元的激活值。下图展示了这些记号的使用：  </p><img src="/2018/09/07/nndl-chapter2/tikz17.png" title="tikz17">  <p>（未完待续）</p><p>其它章节：  </p><p><a href="https://suddenchive.github.io/2018/09/01/nndl-chapter0/">神经网络和深度学习 前言（翻译）</a>   </p><p><a href="https://suddenchive.github.io/2018/09/05/nndl-chapter1/">神经网络和深度学习 第一章（翻译）</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文翻译自 &lt;a href=&quot;http://michaelnielsen.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Michael Nielsen&lt;/a&gt; 的在线书籍 &lt;a href=&quot;http://neuralnetworksanddeeplearning.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Network and Deep Learning&lt;/a&gt;。感谢 Nielson 为我们带来如此全面高质量的内容。  &lt;/p&gt;
&lt;h2 id=&quot;反向传播算法是如何实现的&quot;&gt;&lt;a href=&quot;#反向传播算法是如何实现的&quot; class=&quot;headerlink&quot; title=&quot;反向传播算法是如何实现的&quot;&gt;&lt;/a&gt;反向传播算法是如何实现的&lt;/h2&gt;&lt;p&gt;在上一章中，我们看到神经网络是如何使用梯度下降算法来学习权重和偏置的。但在讲述中遗留了一个问题：我们并没有讨论如何计算代价函数的梯度。这还真是个大问题呢！在这一章中，我会介绍一个快速计算这些梯度的算法，这个算法也被称为反向传播算法。  &lt;/p&gt;
&lt;p&gt;反向传播算法于20世纪70年代被首次提出，但其重要性直到由 &lt;a href=&quot;http://en.wikipedia.org/wiki/David_Rumelhart&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;David Rumelhart&lt;/a&gt;， &lt;a href=&quot;http://www.cs.toronto.edu/~hinton/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Geoffrey Hinton&lt;/a&gt; 和 &lt;a href=&quot;http://en.wikipedia.org/wiki/Ronald_J._Williams&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Ronald Williams&lt;/a&gt; 撰写的一篇著名论文的发表才被世人发觉。这篇论文描述了一些神经网络，在这些网络中，反向传播算法会比之前的学习算法取得更高的效率，从而使得神经网络能解决一些先前无法解决的问题。如今，反向传播算法已经成为神经网络学习的中流砥柱。  &lt;/p&gt;
    
    </summary>
    
    
      <category term="translation" scheme="https://suddenchive.github.io/tags/translation/"/>
    
      <category term="deep learning" scheme="https://suddenchive.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习 第一章（翻译）</title>
    <link href="https://suddenchive.github.io/2018/09/05/nndl-chapter1/"/>
    <id>https://suddenchive.github.io/2018/09/05/nndl-chapter1/</id>
    <published>2018-09-05T02:00:00.000Z</published>
    <updated>2018-09-13T07:52:07.926Z</updated>
    
    <content type="html"><![CDATA[<p>本文翻译自 <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> 的在线书籍 <a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="noopener">Neural Network and Deep Learning</a>。感谢 Nielson 为我们带来如此全面高质量的内容。</p><h2 id="1-使用神经网络来识别手写数字"><a href="#1-使用神经网络来识别手写数字" class="headerlink" title="1. 使用神经网络来识别手写数字"></a>1. 使用神经网络来识别手写数字</h2><p>人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：  </p><img src="/images/nndl-chapter1/digits.png" width="300" height="50" title="digits"> <p>大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主要的视觉皮层，常被称为 $V1$，其包含了 $1.4$ 亿个神经元以及这些神经元之间数百亿个连接。然而人类视觉不仅仅包含 $V1$，还有一整套的视觉皮层—— $V2, V3, V4$ 和 $V5$ ——  以完成逐渐复杂的图像处理任务。我们头上其实是顶了一台超级计算机的，这台超级计算机进过了 $1$ 亿年的进化演变，从而擅长理解视觉的世界。辨认手写的数字从不是一件容易的事情。<a id="more"></a>更准确地说，我们人类意外地擅长于使眼睛展现给我们的东西变得合理。但是这一切工作都是无意识就完成的，所以我们并不会感激我们的视觉系统解决了多么困难的问题。  </p><p>视觉模式识别这一问题的难度在当你尝试编写一个计算机程序来辨认上文中那样的数字时会变得格外明显。我们凭自己做起来很容易的事情突然之间变得很困难。一些简单的如何辨认形状的直观经验，比如 数字 $9$ 顶端是一个圈，而右下部分是一条竖直的线，似乎并不能很容易地在算法中表达。当你试图建立起像这样精确的规则时，你会很快地陷入大量异常、禁止和特殊情况的泥沼之中。看起来很绝望了。  </p><p>神经网络用了一种不同的方式尝试解决这个问题。其核心思想是，取出已有的大量的手写数字示例，也被称为训练示例，  </p><img src="/2018/09/05/nndl-chapter1/mnist_100_digits.png" title="mnist_100_digits">  <p>然后开发一个能从这些示例中学习的系统。换句话说，神经网络利用这些示例来自动地推断出辨认手写数字的规则。更多地，通过增加训练示例的数量，神经网络能够学到更多关于手写的知识，从而提升自身的准确度。所以，就像你看到上面我列举了 $100$ 个训练示例的手写数字那样，或许我们能使用数千甚至上万、上亿级别的训练示例集来建立一个更好的手写数字辨认器。  </p><p>在本章中，我们将编写一个识别手写数字的神经网络的程序，其代码长度只有 $74$ 行，并且没有使用任何神经网络的代码库。这段简短的代码将能以 $96\%$ 的准确率在没有人工干涉的情况下辨认手写数字。更多地，我们在后续章节中将完善我们的思想以完成准确率超过 $99\%$ 的程序。事实上，在商业中神经网络的表现已经足够好了，以至被用于银行处理支票、邮局辨认邮编地址。  </p><p>我们将注重介绍手写数字的识别，因为这是我们在整体上学习神经网络的绝佳的原型问题。作为原型，它取得了一个很好地平衡点：它具有挑战性——即完成辨认手写数字的会是一项不小的成就——但它又没有困难到需要极端复杂的解决方式或者极为庞大的计算能力。更多地，它也是一种衍生出更多更为先进的技术的很好的方式，比如深度学习。所以，在阅读整本书的过程中，我们会不断回到辨认手写数字的问题上来。在之后的内容中，我们也会讨论这些思想将如何被运用到计算机视觉的其他问题中，以及类似的，自然语言处理和其他领域。  </p><p>当然了，如果本章的目的仅仅是编写一个辨认手写数字的计算机程序，那么本章的篇幅将会比现在短很多！所以在这个过程中，我们还会引出许多神经网络的关键思想，包括两种重要的人工神经元（感知器和 Sigmoid 神经元）和标准的神经网络的学习算法，即随机梯度下降算法。自始至终地，我都将关注于解释各个部分的内容为什么要被这样设计，以及建立你关于神经网络的直观感受。这些将需要比我仅仅介绍基本的技巧要多很多的讨论，但为了你能深刻地理解，这一切都是值得的。我们的收获会是，在本章结束之后，能理解深度学习是什么，并且为什么它很重要。   </p><h3 id="1-1-感知器"><a href="#1-1-感知器" class="headerlink" title="1.1. 感知器"></a>1.1. 感知器</h3><p>什么是神经网络？作为开始，我会描述一种人工神经元，它被称为感知器（<strong>Perceptrons</strong>）。感知器于 $1950s - 1960s$ 由 <strong>Frank Rosenblatt</strong> 研究并发展，而 <strong>Rosenlatt</strong> 受启发于先前 <strong>Warren McCulloch</strong> 和 <strong>Walter Pitts</strong> 的工作。现如今，更普遍的是使用其他人工神经元的模型，不管是在本书还是在现代的神经网络中。主要被使用的模型是  sigmoid神经元。我们将会很快介绍到 sigmoid。但为了理解为什么 sigmoid 神经元被设计成那样，我们需要先花时间理解感知器。  </p><p>那么，感知器是如何工作的呢？一个感知器，有若干个二进制输入 $x_1, x_2, …$， 产生一个二进制输出：  </p><img src="/2018/09/05/nndl-chapter1/tikz0.png" title="tikz0">   <p>在上图的例子中，感知器拥有三个输入 $x_1, x_2, x_3$ 。通常它可以有更多或者更少的输入。 <strong>Rossenblatt</strong> 提出了一种简单地计算输出的规则。他用到了权重 $w_1, w_2, … ,$ 这些实数来表示对应的输入对于输出的重要性。神经元的输出为 $0$ 或者 $1$，由加权和 $\sum_{j}w_jx_j$ 是小于还是大于某个阈值来决定。可以用以下代数用语来准确地表示：  </p><script type="math/tex; mode=display">\tag{1} \label{1}\begin{equation}output = \left \{\begin{array}{rcl}  0 & & {if \ \  \sum_j w_j x_j \leq threshold}\\  1 & & {if \ \  \sum_j w_j x_j > threshold}  \end{array}\right.\end{equation}</script><p>这就是感知器工作原理的全部内容了！  </p><p>这是一个基本的数学模型。有一种思考感知器的方式是把它作为通过权衡已知证据来做出决策的装置。让我来举个例子。这不是一个很有现实意义的例子，但却很容易理解，并且我们也将很快会接触到更现实的例子。设想，周末即将来临，而你听说你的城市将举办一场芝士节。你非常喜欢芝士，于是得决定是否要去参加芝士节。你或许会权衡以下三个因素来做决策：</p><ol><li><p>周末的天气会不会很好？  </p></li><li><p>你的男朋友或者女朋友愿意陪你一起么？  </p></li><li><p>芝士节的举办地点离公共交通的站点距离近么？  </p></li></ol><p>我们能用对应的二进制变量 $x_1, x_2, x_3$ 来表示这三个因素。比如，如果周末天气不错，我们就让 $x_1 = 1$ ，如果天气很差，就让 $x_1 = 0$ 。类似的，当男朋友或女朋友也想去的时候，$x_2 = 1$， 反之，则 $x_2 = 0$。对 $x_3$ 和公共交通站点做类似的设置。  </p><p>现在，假设你超超超爱芝士的，爱到即使你的男朋友或女朋友不感兴趣，并且去芝士节的交通也很麻烦，你还是想要参加。但是或许你却无法忍受恶劣的天气，如果天气糟糕的话你就怎么都不愿意参加芝士节了。那么，你能使用感知器来为这类决策建立模型。一种方法是，令代表天气的权重 $w_1 = 6$， 其他因素的权重为 $w_2 = 2$ 和 $w_3 = 2$。$w_1$ 的数值越大，表示天气因素对你而言比男（或女）朋友是否愿意陪同或交通远近都要重要得多。最后，假设你将感知器的阈值设置为 $5$。有了以上的设置，感知器就实现了一个做决策的模型，只要天气好就会输出 $1$，天气差就会输出 $0$ 。男（或女）朋友是否愿意陪同，或是交通的远近，都无关紧要了。  </p><p>通过调整这些权重和阈值，我们能得到不同的决策模型。例如，假设我们将阈值改为 $3$。那么，感知器将权衡天气好坏、男（女）朋友是否愿意陪同和交通的远近来决定要不要参加芝士节。换句话说，它就不是最初那个决策模型了。降低阈值意味着你变得更愿意去参加芝士节。  </p><p>显然感知器才不会是人类决策的完整模型呢！但这个例子体现了一个感知器能如何权衡不同种类的已知因素来做决策。并且，让我们有理由相信，一个复杂的感知器网络能做出更加精确的决策：  </p><img src="/2018/09/05/nndl-chapter1/tikz1.png" title="tikz1">   <p>在这个网络中，第一列感知器——我们也称之为第一层感知器——通过权衡输入的因素来做出非常简单的决策。那第二层的感知器呢？这些感知器中的每一个都权衡第一层感知器决策的结果，从而做出决策。通过这样的方式，第二层的感知器能在与第一层感知器相比更复杂和抽象的层面上做出决策。同理，第三层的感知器又能做出更复杂的决策。这样，一个多层的感知器网络就能从事复杂又巧妙的决策制定。  </p><p>顺便提及，当我定义感知器时，我说过，一个感知器只有一个输出。而在上述感知器的网络中，看上去它们每个都有多个输出。事实上，它们仍然只有一个输出。多个输出箭头只是一种表明该感知器的输出被用来作为其它多个感知器的输入值的方式。这种方式比画一条输出箭头然后分裂出多条来要显得不那么奇怪吧。  </p><p>接下来，让我们简化一下对感知器的描述。条件 $\sum_j w_j x_j &gt; threshold$ 未免显得太过笨重，我们通过改变两处记号来简化它，首先，用点乘的形式 (dot product) 来简写 $\sum_j w_j x_j$，即令 $w \cdot x = \sum_j w_j x_j$，其中$w$ 和 $x$ 分别代表由权重和输入构成的向量；第二个改变是，将阈值移到不等式的另一边，并且用感知器的偏置来表示，即令 $b \equiv -threshold$。用偏置来代替阈值，感知器的规则就能被重写为：  </p><script type="math/tex; mode=display">\tag{2} \label{2}\begin{equation}output = \left\{    \begin{array}{rcl}    0 & & {if \ \ w \cdot x + b \leq 0} \\    1 & & {if \ \ w \cdot x + b > 0}    \end{array}\right.\end{equation}</script><p>你可以认为偏置是让感知器输出 $1$ 的容易程度。或者用更生物的用语来表达，即偏置是让感知器激活的容易程度。对于一个拥有很大偏置的感知器，非常容易就让它输出 $1$ 了，但如果偏置是一个非常大的负数，那么这个感知器就很难输出 $1$。显然，引入偏置只是我们描述感知器时做出的小小的改变，但稍后我们会看到它能带来更多的记号上的简化。也因为如此，在本书剩下的部分，我们都不会使用阈值，而是改为使用偏置这个概念。  </p><p>我已经将感知器描述为一种权衡各种因素来做决策的方法。而感知器的另一个用途是计算基础的逻辑函数，即一些我们认为是底层的计算，像 AND， OR 和 NAND。例如，假设我们令 $ w_1 = w_2 = -2, b = 3 $ 。即下面的这个感知器：  </p><img src="/2018/09/05/nndl-chapter1/tikz2.png" title="tikz2">  <p>那么，我们发现，输入 $ 00 $ 将得到输出 $ 0 $ ，因为 $ (-2) \ast 0 + (-2) \ast 0 + 3 = 3 $ 大于 $ 0 $ 。请注意，这里我用了 $\ast$ 符号来表示普通的乘法运算。经过简单的计算，对于输入 $ 01 $ 和 $ 10 $ 也得到 $ 1 $ 。但对于输入 $ 11 $ ，却输出了 $ 0 $ ，因为 $ (-2) \ast 1 + (-2) \ast 1 + 3= -1 $ 小于 $ 0 $ 。所以，我们用这个感知器竟然实现了 NAND 门！  </p><p>NAND 门的例子表明我们能使用感知器来计算简单的逻辑函数。事实上，我们能使用感知器网络来计算任意逻辑函数。原因是，NAND 门对于逻辑运算来说是通用的，也就是说，我们能仅使用 NAND 门来构造所有的逻辑计算过程。举个例子，我们能使用 NAND门来实现将两个比特相加的运算电路。这要求计算位上的加和，即$ x_1 \oplus  x_2 $ ，也要计算进位，例如，当 $ x_1 $ 和 $ x_2 $ 都为 $ 1 $ 时，进位才为 $ 1 $ 。进位的值也就是位上相乘的值，即 $ x_1 x_2 $ ：  </p><img src="/2018/09/05/nndl-chapter1/tikz3.png" title="tikz3">  <p>为了得到等价的感知器网络，我们将所有的 NAND 门替换为有两个输入的感知器，并且其权重都为 $ -2 $ ，偏置为 $ 3 $ 。下图为替换后得到的网络。注意到，我稍微移动了一下对应于右下角 NAND 门的感知器，这样做仅仅是为了更容易地画图中的箭头符号。  </p><img src="/2018/09/05/nndl-chapter1/tikz4.png" title="tikz4">  <p>这个感知器网络中一个吸引注意的地方是，最左端的感知器的输出作为最底端感知器的输入居然使用了两次。当我定义感知器模型的时候，我没说这种重复输出到同一个地方的情况是否被允许。但其实，这并不重要。如果我们不想允许这种情况，那么就简单地将两条代表输出的线——每条线上的权重都为 $ -2 $ ——合并成一条，然后把权重设置为 $ -4 $ 。（如果你没法显然得到这个等价的变换，你应该停下来先尝试证明一下。）经过这个修改，网络将会变成下面这个样子，所有没被标注出来的权重都是 $ -2 $ ，所有偏置都是 $ 3 $ ，其中有一个权重为 $ -4 $ ：  </p><img src="/2018/09/05/nndl-chapter1/tikz5.png" title="tikz5"><p>直到现在，我都把输入 $ x_1 $ 和 $ x_2 $ 当做变量悬放在感知器网络的左端。事实上，传统地做法是额外地画一层感知器——输入层——来对输入进行编码：  </p><img src="/2018/09/05/nndl-chapter1/tikz6.png" title="tikz6">  <p>这种只有一个输出而没有输入的输入感知器的记号，  </p><img src="/2018/09/05/nndl-chapter1/tikz7.png" title="tikz7">  <p>是一种简写。它并不真的表示存在一个没有输入的感知器。为了解释这一点，假设我们确实有这样的感知器，它不接受任何输入。那么，加权和 $ \sum_j w_j x_j $ 会一直等于 $ 0 $ ，于是如果 $ b &gt; 0 $ 那么感知器就会输出 $ 1 $ ，$ b &lt; 0 $ 则输出 $ 0 $ 。也就是说，这个感知器仅仅会简单地输出一个修正值，而不是我们想要的值（上例中的 $ x_1 $ ）。最好认为输入感知器并不真的是一个感知器，而是一个简单的能输出我们期望的值 $x_1, x_2, …$ 的特殊元件。  </p><p>加法器的例子向我们展示了一个感知器网络能被用来模拟包含很多 NAND 门的电路。而因为 NAND 门具有逻辑计算的通用性，也就得出感知器也具有计算通用性的结论。  </p><p>了解到感知器的计算通用性是同时让人宽慰又失望的，宽慰是因为知道感知器网络能与其他计算设备一样强大；但失望是因为，看起来，感知器只不过是 NAND 门的另一种形式，这样的话感知器可难以称得上是一个大新闻！  </p><p>然而，情况比听起来的要好一些。事实表明，我们能够部署学习算法，来自动地调整人工神经元网络中的权重和偏置。作为对外部刺激的响应，这种调整无须编程者的干涉就能产生。这些学习算法使得我们能通过一种从根本上不同于传统逻辑门的方式来使用人工神经元。区别于单纯设计一个由 NAND 和其他逻辑门组成的电路，我们的神经元网络简单地通过学习来解决问题，有时这些问题如果直接用设计传统电路的方式会变得异常困难。  </p><h3 id="1-2-Sigmoid-神经元"><a href="#1-2-Sigmoid-神经元" class="headerlink" title="1.2. Sigmoid 神经元"></a>1.2. Sigmoid 神经元</h3><p>能学习的算法，听起来很吸引人不是嘛。但是会我们如何将这样的算法应用到一个神经网络中呢？假设我们有一个感知器网络，并且我们希望让它能学习解决某些问题。例如，这个网络的输入或许是来自一个手写数字的影印图片的行像素数据。我们希望网络学习权重和偏置以使得网络的输出能够正确地分类手写数字。为了观察学习的过程，假设我们对网络中的权重或偏置进行很小的修改。我们想要的结果是，对于这个很小的修改，仅会相应地引起网络的输出发生很小的变化。接下来我们会看到，这一性质将使得学习变为可能。大致上，我们想要的是下面这个样子（显然这对于完成手写数字的识别有点过于简单了！）：  </p><img src="/2018/09/05/nndl-chapter1/tikz8.png" title="tikz8"><p>如果对权重（或偏置）做出很小的修改时仅仅会引起输出的很小的改变，那么我们能利用这一点来修改权重和偏置来让网络的行为变得更像我们希望的那样。例如，假设网络错误地将数字 “$9$” 的图片识别为数字 “$8$” 。我们能计算出对权重和偏置做出多小的修改时，会让网络离将这张图片识别为正确地数字 “$9$”更近一步 。然后我们重复这个过程，不断地修改权重和偏置从而产生越来越好的输出。这就是网络的学习过程。  </p><p>问题是，如果我们的网络中包含感知器的话，以上的情况将不会发生。事实上，网络中的任意单个感知器，对于权重或偏置的很小的变化，有时会引起该感知器输出值的跳跃，比如，从 $0$ 变为 $1$ 。这个跳跃也许会造成网络的其他部分以一种复杂的方式发生完全的改变。所以，尽管现在 “$9$” 能被正确地分类，网络对于所有其他图片的识别结果也会以很难控制地方式完全地改变。这会让我们通过逐渐修改权重和偏置使得网络更靠近期望的行为变得困难。或许存在一些聪明的方法来避开这一问题。但如何让一个感知器网络达到学习的效果似乎还是毫无头绪。  </p><p>我们可以引入一种新的被称为 sigmoid 神经元的人工神经元来解决这一问题。Sigmoid 神经元类似于感知器，但也做了一些修改，从而使得对权重和偏置的细小修改只会对输出造成很小的改变。这是保证Sigmoid 神经元网络能够学习的关键因素。  </p><p>好的，那么让我来描述一下 sigmoid 神经元。我们将用与描述感知器一样的方式来表示 Sigmoid 神经元：  </p><img src="/2018/09/05/nndl-chapter1/tikz9.png" title="tikz9">  <p>就像感知器，sigmoid 神经元也有输入 $x_1, x_2, … $。但与输出 $ 0 $ 或者 $ 1 $ 不同，这些输入在 sigmoid 神经元上产生的可以是介于 $ 0 $ 和 $ 1 $ 之间的任意值。例如，$ 0.638 … $ 就是一个 sigmoid 神经元有效的输出。与感知器还类似的一点是， sigmoid 神经元对于每个输入，也有相应的权重 $ w_1, w_2, … $ ，和一个总体的偏置 $ b $ 。Sigmoid 的输出不是 $ 0 $ 或者 $ 1 $ ，而是 $ \sigma (w \cdot x + b) $ ，其中 $ \sigma $ 被称为 sigmoid 函数* <span class="marginnote">* 顺便提下，$ \sigma $ 有时也被称为逻辑函数，这种新的神经元的类型是逻辑神经元。记住这个术语很有用，因为这些术语被很多与神经网络打交道的人使用。然而，我们还是先专注于 sigmoid 。</span>，其定义如下：</p><script type="math/tex; mode=display">\tag{3} \label{3}\sigma(z) \equiv \frac{1}{1 + e^{-z}}.</script><p>更准确地，一个输入是 $x_1, x_2, …$，权重为 $w_1, w_2, …$ ，偏置为 $ b $ 的 sigmoid 神经元的输出为  </p><script type="math/tex; mode=display">\tag{4} \label{4}\frac{1}{1 + {\rm exp}(-\sum_{j}{w_j x_j} - b)}.</script><p>一眼看上去，sigmoid 神经元的长相跟感知器很不一样。Sigmoid 函数的代数形式看起来很不清晰明了，并且如果你对它不熟悉的话也会望而生畏。事实上，sigmoid 神经元与感知器之间有很多相似之处，另外，sigmoid 函数的代数形式更多地是技术细节而不是你理解上的障碍。  </p><p>为了理解其与感知器之间的相似处，假设 $ z \equiv w \cdot x + b $ 是一个很大的正数。那么 $ e ^ {-z} \approx 0 $ ，从而 $ \sigma (z) \approx 1 $ 。换句话说，当 $ z = w \cdot x + b $ 数值很大且符号为正，那么 sigmoid 神经元的输出近似为 $ 1 $ ，就跟感知器上发生的一样。相反地，假设 $ z = w \cdot x + b $ 是非常小的负数。那么 $ e ^ {-z} \rightarrow \infty $ ，从而 $ \sigma (z) \approx 0 $ 。所以，当 $ z = w \cdot  x + b $ 数值很大且符号为负，那么 sigmoid 神经元的行为也与感知器很相近。只有当 $ w \cdot x + b $ 的数值不太大的时候，才与感知器模型有很大的不同。  </p><p>那么关于 $ \sigma $ 的代数形式呢？我们又应该怎么理解它？事实上， $ \sigma $ 的具体形式不那么重要 —— 重要的是画出来的这个函数的图形。如下：  </p><img src="/2018/09/05/nndl-chapter1/tikz9-1.png" title="tikz9-1">  <p>这个图形是如下阶跃函数的平滑版：  </p><img src="/2018/09/05/nndl-chapter1/tikz9-2.png" title="tikz9-2">  <p>如果 $ \sigma $ 被定义为阶跃函数，那么 sigmoid 神经元就会成为一个感知器，因为其会根据 $ w \cdot x + b $ 的正负号来输出 $ 1 $ 或 $ 0 $<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> 。  </p><p>通过使用真正的 $ \sigma $ 函数，我们就能得到上述的一个平滑过的感知器。确实，$ \sigma $ 函数的平滑性才是关键所在，而不是它具体的形式。$ \sigma $ 的平滑性意味着，权重的细小改变 $ \triangle  w_j $ 与偏置的细小改变 $ \triangle b $ 将使得网络的输出产生一个细小改变 $ \triangle \rm output $ 。事实上，由微积分学理论可以知道，$ \triangle \rm output $ 能被很好地近似求得：  </p><script type="math/tex; mode=display">\tag{5} \label{5}\triangle {\rm output} \approx \sum_{j}{\frac{\partial \ {\rm output}}{\partial w_j} \triangle w_j} + \frac {\partial \ {\rm output}}{\partial b} \triangle b,</script><p>其中，求和是针对所有的权重 $ w_j $，而 $ \partial  {\rm output} / \partial w_j $ 和 $ \partial  {\rm output} / \partial b $ 表示 $ \rm output $ 分别关于 $ w_j $ 和 $ b $ 的偏微分。如果你不对付偏微分的话也不要感到惊慌！虽然上面这个表达式看起来很复杂，有一堆偏微分，它其实只是表达了很简单的一件事儿（而且是件很棒的事儿）：$ \triangle \rm output $ 是关于权重和偏置的变化，即 $ \triangle w_j $ 和 $ \triangle b $ 的线性函数。这种线性关系使得选择对权重和偏置进行细小修改从而让输出达到任意想要的细小改变这件事变得相当容易。所以即使 sigmoid 神经元与感知器在性质和行为上有很多相似之处，但它让计算权重和偏置的改变会如何影响其输出变得简单了许多。  </p><p>如果真的如之前所说， $ \sigma $ 的图像才是关键，而不是它的具体形式，那么我们为什么将 $ \sigma $ 定义为 $ \eqref{3} $ 这样呢？事实上，在本书后面的内容，我们会偶尔考虑其他类型的神经元，这些神经元的输出同样为 $ f( w \cdot x + b) $，只不过激活函数 $ f $ 的形式不同。当我们使用不同的激活函数，唯一有区别的一点是， $ \eqref{5} $ 中偏微分的具体值发生了改变。而经验表明，若使用现在的 $ \sigma $ 的定义，能简化计算这些偏微分的过程，因为指数函数求导时有一些迷人的性质。总之， $ \sigma $ 在神经网络中被普遍地使用，本书也用的最多。  </p><p>我们如何解释一个 sigmoid 神经元的输出呢？显然，感知器和 sigmoid 神经元的一个很大的区别在于，sigmoid 神经元不只输出数值 $ 0 $ 和 $ 1 $ ，类似 $ 0.173… $ 或 $ 0.689… $ 等等都是合法的输出。这一点很有用处，比如说，当我们想让输出值表示为输入到神经网络的一张图片上所有像素点的平均亮度。但有时这一特性又是件麻烦事。假设我们想让网络的输出指示“输入图片是张 $ 9 $” 或者“输入图片不是张 $ 9 $”。显然，如果输出只能是 $ 0 $ 或 $ 1 $ ，就像感知器，这件事会简单得多。  </p><p>但实践中，我们能人工设置一个约定来解决，例如，对于大于 $ 0.5 $ 的任意输出，都指示“输入图片是张 9”，而小于 $ 0.5 $的任意输出，都指示“不是 9“。每当我们使用类似约定时，我都会明确指出，所以不会造成你对此的任何疑惑。  </p><h4 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h4><ul><li><p><strong>用 sigmoid 神经元模拟感知器（第一部分）</strong><br>假设我们将感知器网络中所有的权重和偏置都乘上一个正的常数 $ c &gt; 0$。证明网络的行为不会发生变化。  </p></li><li><p><strong>用 sigmoid 神经元模拟感知器（第二部分）</strong><br>假设起始我们与上题一样，有一个感知器网络。同时假设网络的输入是给定的，这里不需要给定具体的输入值，只要求输入是我们想要的样子。这个网络中的任何感知器，对此输入 $ x $，其权重和偏置都满足 $ w \cdot x + b \neq 0 $ 。现在，将网络中的所有感知器都替换为 sigmoid 神经元，然后将所有权重和偏置都乘上一个正的常数 $ c &gt; 0 $ 。证明当极限 $ c \rightarrow \infty $ 成立时，这个 sigmoid 网络的行为与之前的感知器网络完全相同。而如果有任一感知器满足 $ w \cdot x + b = 0 $ ，这一结论就不成立了，为什么呢？  </p></li></ul><h3 id="1-3-神经网络的架构"><a href="#1-3-神经网络的架构" class="headerlink" title="1.3. 神经网络的架构"></a>1.3. 神经网络的架构</h3><p>在下一节中我将介绍一个能很好地分类手写数字的神经网络。作为准备工作，有必要解释一下我们用来给一个网络的不同部分命名的术语。假设我们有这样的网络：  </p><img src="/2018/09/05/nndl-chapter1/tikz10.png" title="tikz10">  <p>正如之前所说，网络中最左端的一层叫做输入层，并且这层的神经元叫做输入神经元。最右端或者说输出层包含了输出神经元，在这个例子中，只有一个输出神经元。中间那一层被称为隐藏层，因为这层里的神经元既不属于输入也不属于输出。“隐藏”这个词或许听起来有点儿神秘——我第一次听到这个词的时候还想着它肯定有什么深层次的哲学或数学上的重要性——但它真的就是“既不属于输入也不属于输出”的意思。上面这个网络只有一个隐藏层，但有些网络能有多个隐藏层。例如，下面拥有两个隐藏层的四层网络：  </p><img src="/2018/09/05/nndl-chapter1/tikz11.png" title="tikz11">  <p>然而颇具迷惑性的是，由于一些历史原因，像这样的多层网络有时候会被称为多层感知器或 MLPS ( Multilayer Perceptrons ) ，尽管组成它的都是 sigmoid 神经元而不是感知器。我不会在本书中使用 MLP 这个术语，因为这太有迷惑性了，但我想提示你这个术语的存在。  </p><p>网络的输入层和输出层的设计经常都很直接。例如，假设我们想要判断一张图片里的手写数字是否为”$ 9 $“。一种自然的设计方式是，将图片像素的亮度编码到输入神经元中。如果这张图片是一张 $ 64 \times 64 $ 的灰度图，那么我们就有 $ 4096 = 64 \times 64 $ 个输入神经元，并且它们的亮度被准确地缩放到 $ 0 $ 与 $ 1 $ 之间。输出层将只包含单个神经元，其输出值若小于 $ 0.5 $ 则表示“输入图片不是一张 $ 9 $”，若大于 $ 0.5 $，则表示“输入图片是一张 $ 9 $”。  </p><p>尽管网络的输入层和输出层的设计经常都很直接，但隐藏层的设计有时可以显得很有艺术性。尤其是，根本不可能用几条简单的经验法则来总结所有的隐藏层的设计流程。所以，神经网络的研究者们提出了许多设计隐藏层的启发式方法，从而让神经网络表现出人们想要的行为。例如，一些启发式方法能用来帮助决定如何在隐藏层神经元的数量和训练网络所需要的时间中折中。在后面的内容中，我们会遇到几种这样的启发式方法。  </p><p>直到现在，我们已经讨论了前一层神经元的输出会作为后一层的输入这种神经网络。这类网络我们称之为前馈神经网络。这也意味着，网络中不存在环形——信息都是向前传递的，而不会向后。如果有环形的话，会导致 $ \sigma ​$ 函数的输入取决于其输出。对这种情况很难赋予其实际意义，所以我们不允许这样的环形。  </p><p>然而，也有其他的神经网络模型，允许存在向后反馈的环。这些模型被称为循环神经网络。这种模型的基本思想是，让神经元的输出只保持有限的一段时间，之后神经元就陷入静止。其输出也能刺激其他的神经元，或许稍等片刻也能使其激活，同样产生只持续有限时间的输出。这样，刺激更多的神经元输出，经过一段时间，我们就能得到瀑布式的输出传递。环形并不会在这样的模型中带来问题，因为一个神经元的输出只会在过了一段时间之后才影响自身的输入，而不是立即生效。  </p><p>循环神经网络没有前馈网络那样有影响力，部分是因为循环神经网络的学习算法（至少到目前为止）不是那么强大。但循环网络还是极其有吸引力的。它们比前馈网络更接近我们大脑的工作方式。并且有可能，循环神经网络能解决一些前馈网络得用很高的复杂度才能完成的重要问题。但是，量力考虑的话，这本书中我们将只关注更被广泛使用的前馈网络。  </p><h3 id="1-4-一个分类手写数字的简单网络"><a href="#1-4-一个分类手写数字的简单网络" class="headerlink" title="1.4. 一个分类手写数字的简单网络"></a>1.4. 一个分类手写数字的简单网络</h3><p>在定义了神经网络之后，让我们回到手写识别这个问题上来。我们能将识别手写数字的问题分为两个子问题。首先，我们希望有一种方式能将一张包含许多数字的图片分割成一系列只包含一个数字的图片，例如，我们要将这张图片  </p><img src="/images/nndl-chapter1/digits.png" width="300" height="50" title="digits">   <p>分成六张独立的图片  </p><img src="/images/nndl-chapter1/digits_separate.png" width="400" height="50" title="digits_separate">   <p>我们人类很轻松就能解决这个图片的分割问题，但对于一个计算机程序来说，正确地将图片分割开会是一个挑战。一旦这张图片被分割开，之后程序就需要对每一个数字进行分类。举例来说，我们希望我们的程序识别上面的第一个数字，  </p><img src="/images/nndl-chapter1/mnist_first_digit.png" width="100" height="100" title="mnist_first_digit">  <p>为一个 $5$ 。  </p><p>我们将关注如何写一个程序来解决第二个问题，也就是，识别单独的数字。我们这样做因为，事实上分割问题不是那么困难，如果你能够很好地解决分类单独数字的话。有很多种解决分割问题的方法。一种方法是，尝试多种不同的分割图片的方式，对其结果，使用单独数字的分类器来评分。一个图片分割的结果，当单独数字的分类器能对其每个分割段都有确定的分类输出，会得到更高的评分；而当单独数字的分类器在对其中一个或者几个分割段遇到分类的困难时，这个分割方式就只能得到一个低分。其思想就是，如果分类器在某个地方遇到困难，可能是因为分割方式选择得不正确。这个思想和它的一些变种在用来解决分割问题上的效果还不错。所以与其担心分割问题，我们将集中注意力于开发一个神经网络来解决更有趣和困难的问题，亦即，识别单独的手写数字。  </p><p>为了识别单独的数字，我们将会使用三层结构的神经网络：  </p><img src="/2018/09/05/nndl-chapter1/tikz12.png" title="tikz12">  <p>这个网络的输入层包含了编码输入像素点值的神经元。正如下一节将讨论的一样，对于这个网络，我们的训练数据将由许多 $ 28  \ast 28 $ 分辨率的手写数字的扫描图片，所以输入层包含  784 = 28 \ast 28  个神经元。为了简便，我在上图中省略了大部分的输入神经元。输入的像素值是灰度缩放值，即值为 $ 0.0 $ 时，表示白色，值为 $ 1.0 $ 时，表示黑色，中间的值表示灰度逐渐变黑。  </p><p>网络的第二层是隐藏层。我们用 $ n $ 表示隐藏层的神经元个数，并且将对 $ n $ 取不同的值来试验。这个示例中展示的是一个规模很小的隐藏层，只包含了 $ n = 15 $ 个神经元。  </p><p>网络的输出层包含 $ 10 ​$ 个神经元。如果第一个神经元被激活，比如 输出 $ output \approx 1​$ ，那么表明网络认为这个数字是一个 $ 0 $ 。如果第二个神经元激活那么表明这个网络认为这个数字是一个 $ 1 $ 。以此类推。更精确地说，我们用 $ 0 $ 到 $ 9 $ 来依次标记这些输出神经元，然后找出哪一个有最高的输出值。假如说，是标记为 $ 6 $ 的神经元，那么我们的网络将猜这个数字是 $ 6 $。对其他输出神经元也一样。  </p><p>或许你会想，为什么我们要用 $ 10 $ 个输出神经元。毕竟，网络的目标是告诉我们哪个数字 $(0, 1, 2, … , 9)$ 对应着输入图像。很自然地，我们能使用仅 $4$ 个输出神经元就能做到，只要将这四个神经元的每一个都当做二进制值，其数值取决于该神经元的输出是更接近 $0$ 还是 $1$ 。四个神经元来编码网络对我们的回答足够了，因为 $2^4 = 16$ ，比输入可能的 $10$ 个数值要多很多。那为什么我们反而要用 $10$ 个神经元呢。这不是很浪费么？根本原因来自于经验主义：我们可以都尝试这两种网络的设计，然后会看到，对于这个特定的问题，拥有 $10$ 个输出神经元的网络识别数字的学习效果比拥有 $4$ 个输出神经元的网络要好。但这又会让我们产生疑惑，为什么使用 $10$ 个输出神经元就会更好呢。有没有什么启发式的方法能提前告诉我们应该用 $10$ 个输出神经元来编码而不是 $4$ 个呢？  </p><p>为了理解为什么这样做，可以遵循第一条原则，即弄清楚神经网络到底在做什么。考虑第一种情况，我们使用 $10$ 个输出神经元。我们来关注第一个输出神经元，即试图确定输入数字是否为 $0$ 的神经元。它通过权衡来自隐藏层神经元的输出来做决策。那么那些隐藏层神经元在做什么呢？方便讨论起见，假设隐藏层的第一个神经元是来检测如下这张图片是否存在：  </p><img src="/images/nndl-chapter1/mnist_top_left_feature.png" width="100" height="100" title="mnist_top_left_feature">  <p>提高输入图片与上图重叠部分的像素点的权重，而对于其他像素点，取较小的权重，这个神经元就能做到这一点。同样地，方便起见，我们假设隐藏层的第二、三和四个神经元分别检测下面这些图片是否存在：  </p><img src="/images/nndl-chapter1/mnist_other_features.png" width="300" height="100" title="mnist_other_features">  <p>正如你所猜测的，这四张图片组合成之前一行数字中数字 $0$ 的图像：  </p><img src="/images/nndl-chapter1/mnist_complete_zero.png" width="100" height="100" title="mnist_complete_zero">  <p>所以如果这四个隐藏层神经元全都被激活，那么我们就能得出结论，这个数字就是 $0$ 。当然，我们得出这种结论的方法肯定不止这一种——还有许多其他正当地的方法（就比如，通过将上述图像进行平移或者轻微地变换）。但看起来认定现在这种情况下我们得出输入是 $0$ 的结论还是不会有错的。  </p><p>假设神经网络是这样工作的，那么我们就能对为什么要有 $10$ 个输出神经元，而不是 $4$ 个，给出一个合理的解释了。如果我们只有 $4$ 个输出神经元，那么第一个输出神经元将要试图确定输入数字的最高位的数值是多少。而将最高位的数值和上面所展示的这种简单图形对应起来可不是一件容易的事儿。很难想象有什么好的历史原因让我们将数字的部分形状与它的最高位数值联系起来。  </p><p>现在，说的这些，也只是一个启发式的假想。没有证据表明，三层结构的神经网络就像我所描述的那样工作，即隐藏层神经元来检测简单的部分形状。或许存在一个聪明的学习算法能找出权重的数值从而让我们能只使用 $4$ 个输出神经元。但作为一个启发式的想法，思考我所描述的内容能起到很好的效果，也能让你在设计好的神经网络架构上省去很多的时间。  </p><h4 id="练习-1"><a href="#练习-1" class="headerlink" title="练习"></a>练习</h4><ul><li>有一种方法，通过在上述三层结构的网络中增加一层额外的神经元从而能够确定输入数字的二进制表达。这个额外的神经层将前一层的输出转换成一个二进制表达，如下图所示。找到这样的输出层的一组权重和偏置。假设前三层神经元能使得第三层（也就是之前的输出层）的正确输出的激活值至少大于 $0.99$，而不正确的输出，其激活值低于 $0.01​$ 。  </li></ul><img src="/2018/09/05/nndl-chapter1/tikz13.png" title="tikz13"><h3 id="使用梯度下降来学习"><a href="#使用梯度下降来学习" class="headerlink" title="使用梯度下降来学习"></a>使用梯度下降来学习</h3><p>现在，在设计了我们自己的神经网络之后，怎么让它学习识别数字呢？首先，我们需要的是一个拿来学习的数据集——也被称为训练数据集。我们将使用 <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST 数据集</a>，其包含了几万张手写数字的扫描图片，和它们对应的正确分类。MNIST 这个名字来源于，它是由 <a href="https://en.wikipedia.org/wiki/National_Institute_of_Standards_and_Technology" target="_blank" rel="noopener">NIST</a> （美国国家标准技术研究所）收集的两个数据集上修改得来的。以下是 MNIST 里的一些图片：  </p><img src="/images/nndl-chapter1/digits_separate.png" width="400" height="50" title="digits_separate">  <p>正如你所看到的，这些数字事实上与本章刚开始作为识别的任务所展示的那些数字一样。当然，测试神经网络的时候，我们会让它来识别那些不在我们的训练集中的图片！  </p><p>MNIST 数据集分为两个部分。第一部分包含 $60,000$ 张图片，被用来作为训练数据。这些图片由 $250$ 个人的手写样本扫描得来，其中一半的人是美国人口普查局的员工，另一半是高中学生。这些图片都是灰度图并且分辨率为 $28 \ast 28$。第二部分是用来作为测试数据的 $10,000$ 张图片。同样，它们也是 $28 \ast 28$ 的灰度图。我们将使用这些测试数据来评估我们的神经网络学习识别数字的效果好坏。为了让这种性能测试更有效，测试数据取自与训练数据不同的 $250$ 个人（即使他们也还是人口普查局员工和高中学生）。这能让我们确信，我们的系统能识别那些在训练时没见过的人的手写数字样本。  </p><p>我们将用 $x$ 表示一个训练样本输入。方便起见，将每个训练输入都看作是 $28 \times 28 = 784$ - 维的向量。向量的每个维度都表示图片中对应像素点的灰度值。用 $y = y(x)$ 表示与输入对应的期望输出，其中 $y$ 是一个 $10$-维向量。例如，一个给定的训练图片 $x$，描绘的是数字 $6$，那么， $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$ 就是网络的期望输出。$T$ 是转置操作符，将一个行向量转换成对应的列向量。  </p><p>我们想要的是一个这样的算法，它让我们找到能使得网络对所有训练输入 $x$ 都能近似输出 $y(x)$ 的权重和偏置。为了量化我们达到这个目标的程度，定义一个代价函数*<span class="marginnote">* 有时我们把它称为损失函数或者目标函数。本书中将使用代价函数的说法，但你需要了解其他的术语，因为它们经常会在神经网络的研究论文和讨论中使用到。</span>：</p><script type="math/tex; mode=display">\begin{eqnarray} C(w,b) \equiv \frac{1}{2n} \sum_x \| y(x) - a\|^2. \tag{6}\end{eqnarray}</script><p>其中，$w$ 表示网络中所有权重的集合，$b$ 表示全部的偏置，$n$ 是训练输入的总量，$a$ 是当输入为 $x$ 时网络的输出向量，并且，求和是对于所有训练输入 $x$ 而言的。当然，输出 $a$ 取决于 $x, w$ 和 $b$，但出于书写简洁的考虑我没有展现这一依赖关系。记号 $| v |$ 指的是向量 $v$ 的长度。我们称这样的 $C$ 为二次代价函数，有时也被称为_平均平方误差_或简称为 MSE (mean squared error) 。观察二次代价函数的形式，我们发现，$ C(w, b) $ 的值非负，因为求和的每一项都非负。而且，代价 $ C(w, b) $ 的值会很小，即 $ C(w, b) \approx 0 $，仅当 $ y(x) $ 与输出 $a$ 对于所有的训练输入 $x$ 都近似相等时。所以如果我们的算法找到了使得 $ C(w, b) \approx 0 $ 的权重和偏置时，它的表现就算很好。相反地，如果 $ C(w, b) $ 很大，算法的表现就不佳，因为这意味着对于很多输入， $ y(x) $ 与输入 $a$ 都有很大区别。所以我们的训练算法的目标就是最小化这个关于权重和偏置的代价函数 $ C(w, b) $ 。换句话说，我们想找到一个权重和偏置的集合，使得代价尽可能的小。我们将使用被称为梯度下降的算法来做到这一点。  </p><p>（未完待续）</p><p>其它章节如下：  </p><p><a href="https://suddenchive.github.io/2018/09/01/nndl-chapter0/">神经网络与深度学习 前言（翻译）</a>  </p><p><a href="https://suddenchive.github.io/2018/09/07/nndl-chapter2/">神经网络与深度学习 第二章（翻译）</a> </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文翻译自 &lt;a href=&quot;http://michaelnielsen.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Michael Nielsen&lt;/a&gt; 的在线书籍 &lt;a href=&quot;http://neuralnetworksanddeeplearning.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Network and Deep Learning&lt;/a&gt;。感谢 Nielson 为我们带来如此全面高质量的内容。&lt;/p&gt;
&lt;h2 id=&quot;1-使用神经网络来识别手写数字&quot;&gt;&lt;a href=&quot;#1-使用神经网络来识别手写数字&quot; class=&quot;headerlink&quot; title=&quot;1. 使用神经网络来识别手写数字&quot;&gt;&lt;/a&gt;1. 使用神经网络来识别手写数字&lt;/h2&gt;&lt;p&gt;人类的视觉系统是这个世界上的奇迹之一。考虑如下的手写数字的序列：  &lt;/p&gt;
&lt;img src=&quot;/images/nndl-chapter1/digits.png&quot; width=&quot;300&quot; height=&quot;50&quot; title=&quot;digits&quot;&gt; 
&lt;p&gt;大多数人都能很轻易地辨认出这些数字为 $504192$ 。这种轻易是很具有欺骗性的。在我们大脑的每个半球，都有一个主要的视觉皮层，常被称为 $V1$，其包含了 $1.4$ 亿个神经元以及这些神经元之间数百亿个连接。然而人类视觉不仅仅包含 $V1$，还有一整套的视觉皮层—— $V2, V3, V4$ 和 $V5$ ——  以完成逐渐复杂的图像处理任务。我们头上其实是顶了一台超级计算机的，这台超级计算机进过了 $1$ 亿年的进化演变，从而擅长理解视觉的世界。辨认手写的数字从不是一件容易的事情。
    
    </summary>
    
    
      <category term="translation" scheme="https://suddenchive.github.io/tags/translation/"/>
    
      <category term="deep learning" scheme="https://suddenchive.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>神经网络和深度学习 前言（翻译）</title>
    <link href="https://suddenchive.github.io/2018/09/01/nndl-chapter0/"/>
    <id>https://suddenchive.github.io/2018/09/01/nndl-chapter0/</id>
    <published>2018-09-01T10:30:58.000Z</published>
    <updated>2018-09-13T07:52:05.506Z</updated>
    
    <content type="html"><![CDATA[<p>本文翻译自 <a href="http://michaelnielsen.org/" target="_blank" rel="noopener">Michael Nielsen</a> 的在线书籍 <a href="http://neuralnetworksanddeeplearning.com" target="_blank" rel="noopener">Neural Network and Deep Learning</a>。感谢 Nielson 为我们带来如此全面高质量的内容。  </p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>神经网络和深度学习 是一本免费的在线书籍。这本书将讲授这些内容：  </p><ul><li><p>神经网络，一种漂亮的受生物启发的编程范式，它能使得一台计算机从观测数据中进行学习；  </p></li><li><p>深度学习，一套用于神经网络进行学习的强有力的技术集合。  </p></li></ul><p>神经网络和深度学习如今提供了图像识别、语音识别和自然语言处理中许多问题的最佳解决方案。这本书将向你阐述神经网络和深度学习背后的众多核心概念。  </p><p>关于这本书更多的信息，请看这里。或者你也能直接阅读第一章，开始本次旅程。<br><a id="more"></a>  </p><h2 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h2><p>神经网络是所有发明出的最漂亮的编程范式之一。在传统的编程方法中，我们告诉计算机要做什么，将大的问题拆分为许多细粒度的、精确定义的计算机能轻松完成的任务。不同的是，在神经网络中，我们不需要告诉计算机如何解决我们的问题。而是，它从观测数据中学习，自行找到该问题的解决方法。  </p><p>能自动地从数据中学习，听起来很刺激。然而，直到 $2006$ 年，我们都不知道该如何训练神经网络以超过更传统的方式，只有少数几个极其特别的问题除外。$2006$ 年，让这一切发生改变的，是我们称之为深度神经网络中的一系列新的学习技术的出现。这些技术现在以深度学习的名字广为人知。人们对它们也进行了更多的研究，如今深度神经网络和深度学习在计算机视觉、语音识别和自然语言处理领域的许多问题上都有着卓越的表现。并且，它们也被包括谷歌、微软和脸书在内的很多公司大规模部署。  </p><p>这本书的目的是让你掌握神经网络的核心概念，包括深度学习的最新技术。在读完这本书之后，你能编写出神经网络和深度学习的代码来解决复杂的模式识别问题。并且你也有了基础来使用神经网络和深度学习攻克你自己设计的问题。  </p><h3 id="以原理为目标的方法"><a href="#以原理为目标的方法" class="headerlink" title="以原理为目标的方法"></a>以原理为目标的方法</h3><p>本书的基本信念是，我们希望的是对神经网络和深度学习的核心原则能建立一种完善、全面的理解，而不是仅仅获得对一堆又长又枯燥的想法的模糊的认识。如果你能很好地理解那些核心思想，你也能快速地了解其他部分。以编程语言作类比，就像是掌握了一种新语言的核心语法、库和数据结构。你或许仍只是知道整个语言的一小部分——许多语言都有着庞大的标准库——但新的库和数据结构都能被快速轻易地掌握。  </p><p>这意味着，本书绝不是一本关于如何使用具体神经网络工具库的教程。如果你最想要的是学习关于工具库的知识，请不要阅读本书！找到你想学的代码库，然后啃它的教程和文档。但请注意。虽然能够取得迅速解决问题的好处，如果你想理解神经网络到底在干什么，如果你想具备不随时间流逝的洞察力，那么仅仅学习那些热门的工具库是远不够的。你需要理解在神经网络背后的那些经久不衰的思想。技术只是过客，而思想永恒。  </p><h3 id="结合实践的方法"><a href="#结合实践的方法" class="headerlink" title="结合实践的方法"></a>结合实践的方法</h3><p>我们会通过攻克一个实际问题来学习神经网络的核心理念，即教会计算机识别手写数字。这一问题使用传统方法来编程解决的话会非常困难。但，正如我们会看到的，使用一个简单的神经网络却能很好地解决，仅仅需要数十行的代码并且是在不用某个代码库的前提下。更多地，我们还会通过几轮迭代来提升这个程序的效果，逐渐聚合越来越多的神经网络和深度学习的核心思想。  </p><p>这个手把手的方法意味着你需要一些编程经验才能阅读这本书。但你不必要时一名职业的程序员。我用 Python 2.7 写的代码，但即使你不用 Python 编程，也只需很少的努力就能理解。在本书的课程中，我们会研究一些神经网络的代码库，你能用它们来做实验或者帮助你来理解。所有的代码都能在这里下载到。一旦你读完这本书，你就能轻易地从更完善的神经网络代码库中选一个来用于生产需要了。  </p><p>还有一点相关的，阅读这本书的数学要求是很节制的。在大多数章节都有一些数学内容，但通常只是写基础的代数和函数的图形，我认为大部分读者都能接受的程度。我偶尔会使用一些更艰深的数学，但都以结构化的形式展现，所以即使数学细节让你感到迷惑，你也能懂得它的含义。大量使用难度较高的数学的章节是第二章，需求一定量的多元微分学和线性代数的知识。如果你对这些都不太熟悉，我在第二章的开始讨论了如何避免阅读具体的数学内容。假如你觉得真的很难跟上，你能跳过其余部分，直接阅读该章节主要结论的总结部分。总之，你没必要一开始就太过担心这些。  </p><p>一本书同时以思想原理和手把手实践为目标，是很少见的。但我相信，如果我们建立起关于神经网络的基本思想之后你能学得很好。我们会一起开发可运行的代码，而不仅仅是讨论抽象的理论，这些代码你都能自行研究并进行扩展。这样，你能理解包括理论和实践在内的基础内容，也使得你能更容易地接受新的知识。</p><p>其它章节：  </p><p><a href="https://suddenchive.github.io/2018/09/05/nndl-chapter1/">神经网络和深度学习 第一章（翻译）</a>  </p><p><a href="https://suddenchive.github.io/2018/09/07/nndl-chapter2/">神经网络和深度学习 第二章（翻译）</a>  </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文翻译自 &lt;a href=&quot;http://michaelnielsen.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Michael Nielsen&lt;/a&gt; 的在线书籍 &lt;a href=&quot;http://neuralnetworksanddeeplearning.com&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Network and Deep Learning&lt;/a&gt;。感谢 Nielson 为我们带来如此全面高质量的内容。  &lt;/p&gt;
&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;神经网络和深度学习 是一本免费的在线书籍。这本书将讲授这些内容：  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;神经网络，一种漂亮的受生物启发的编程范式，它能使得一台计算机从观测数据中进行学习；  &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;深度学习，一套用于神经网络进行学习的强有力的技术集合。  &lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;神经网络和深度学习如今提供了图像识别、语音识别和自然语言处理中许多问题的最佳解决方案。这本书将向你阐述神经网络和深度学习背后的众多核心概念。  &lt;/p&gt;
&lt;p&gt;关于这本书更多的信息，请看这里。或者你也能直接阅读第一章，开始本次旅程。&lt;br&gt;
    
    </summary>
    
    
      <category term="translation" scheme="https://suddenchive.github.io/tags/translation/"/>
    
      <category term="deep learning" scheme="https://suddenchive.github.io/tags/deep-learning/"/>
    
  </entry>
  
</feed>
